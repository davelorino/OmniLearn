# OmniLearn

This repository spins up a fully-local **Personal Learning OS** stack.

* **Postgres + pgvector** for structured data  
* **Qdrant** for dense-vector search  
* **FastAPI** backend  
* **llama-cpp**-powered chat model  

using nothing but Docker Compose and one small Python virtual-env.

---

## ‚öôÔ∏è Architecture Overview

```mermaid
flowchart TD
  %% groupings
  subgraph Client Tier
    UI["React / Next.js UI (future)"]
  end

  subgraph API Tier
    A["FastAPI<br/>Uvicorn"]
  end

  subgraph Model Tier
    L["llama-cpp<br/>TinyLlama 1.1 B"]
  end

  subgraph Data Tier
    P["Postgres + pgvector"]
    Q["Qdrant 1.7.3"]
  end

  subgraph Volumes
    V1["pgdata_named"]
    V2["qdrant_named"]
    V3["modelcache_named"]
  end

  %% edges
  UI -- "REST / WebSockets" --> A
  A  -- SQLModel --> P
  A  -- "HTTP / gRPC" --> Q
  A  -- "OpenAI-compatible HTTP" --> L

  P -- persists --> V1
  Q -- persists --> V2
  L -- read-only --> V3
```

---

## üöÄ Feature Set & Development Roadmap

| Phase | Area | Key deliverables | Status |
|-------|------|------------------|:------:|
| **0** | **Scaffold hardening** | One-shot `setup.sh`, named volumes, full tear-down/rebuild sanity-check | ‚úÖ done |
| **1** | **Domain modelling** | SQLModel entities (`User`, `Interest`, `SkillNode`, `AssessmentItem`, etc.), Alembic bootstrap, YAML‚ÜíDB loader | ‚è≥ in&nbsp;progress |
| **2** | Embedding & ingestion | Worker that chunks trusted sources ‚Üí embeddings ‚Üí Postgres & Qdrant | ‚óªÔ∏é |
| **3** | Authoring API | CRUD for skill trees, sources, assessment items (`/author/*`) | ‚óªÔ∏é |
| **4** | Assessment engine | Spaced-repetition scheduler, randomised quizzes, `/grade/*` endpoints | ‚óªÔ∏é |
| **5** | Progress & forecasting | Mastery tracking, learning-velocity model, `/progress/*` endpoints | ‚óªÔ∏é |
| **6** | Front-end MVP | React dashboard: global overview, interest drill-down, charts | ‚óªÔ∏é |
| **7** | AI authoring | RAG-powered skill-tree & assessment generation with human-in-the-loop approval | ‚óªÔ∏é |
| **8** | Portability & DevOps | CI pipeline, health-checks, configurable secrets, multi-arch images | ‚óªÔ∏é |

> *Legend*‚ÄÇ‚úÖ done‚ÄÉ‚è≥ in progress‚ÄÉ‚óªÔ∏é planned

---

## 1 Project at a glance

Layer | Image / Service | Purpose
------|-----------------|---------
Postgres + pgvector | `ankane/pgvector:latest` | relational data + vector column support
Qdrant v1.7.3 | `qdrant/qdrant:v1.7.3` | high-performance ANN vector DB with `/healthz`
FastAPI | *custom build* | business-logic API ‚Äì served by Uvicorn
LLM | `ghcr.io/abetlen/llama-cpp-python:v0.2.79` | runs `tinyllama-1.1B-chat-v1.0.Q4_K_M.gguf` in llama-cpp

All containers store data on **named, bind-mounted volumes** so you can wipe Docker without losing embeddings or database state.

---

## 2 Prerequisites

1. **Docker Desktop** (‚â• 4.29) with File Sharing enabled for `/Volumes/LaCie` (or your own external drive).  
2. **Python ‚â• 3.11** (only to create a light virtual-env and run helper scripts).  
3. **huggingface-hub CLI**

```bash
pip install --upgrade huggingface-hub
```

---

## 3 Quick Start Instructions

```bash
# 0Ô∏è‚É£  Pick an empty folder for your project and cd into it
bash <(curl -sSL <YOUR-RAW-GIST-URL>/setup.sh)   # or copy setup.sh into the dir

source .venv/bin/activate         # enter the virtual environment
docker compose up -d              # build & start all four services
```

`setup.sh` will:

* scaffold the repo (core code, API stubs, tests, etc.),  
* build the `docker-compose.yml` shown below,  
* create a local `model-cache/` directory and prompt you to download TinyLlama, and  
* initialise named volumes (`pgdata_named`, `qdrant_named`, `modelcache_named`).

---

## 4 Model download

```bash
huggingface-cli download \
  TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF \
  tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
  --local-dir ./model-cache \
  --local-dir-use-symlinks False      # copy, don‚Äôt symlink
```

Then:

```bash
docker compose restart vllm
open http://localhost:8001/v1/models     # TinyLlama should appear
```

---

## 5 Health Checks

```bash
# FastAPI
curl -s http://localhost:8000/health                   # {"status":"ok"}

# Postgres
docker compose exec postgres pg_isready                # accepting connections

# Qdrant
curl -s http://localhost:6333/healthz                  # healthz check passed

# LLM
curl -s http://localhost:8001/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
           "model":"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
           "messages":[{"role":"user","content":"Hello!"}]
         }'
```

If Postgres ever complains that `/var/lib/postgresql/data` ‚Äúexists but is not empty‚Äù, re-initialise the volume (remove files, keep the mount).

---

## 6 `docker-compose.yml` (generated by `setup.sh`)

```yaml
services:
  postgres:
    image: ankane/pgvector:latest
    environment:
      POSTGRES_PASSWORD: pass
    volumes: [pgdata_named:/var/lib/postgresql/data]
    ports: ["5432:5432"]

  qdrant:
    image: qdrant/qdrant:v1.7.3
    volumes: [qdrant_named:/qdrant/storage]
    ports: ["6333:6333"]

  vllm:
    image: ghcr.io/abetlen/llama-cpp-python:v0.2.79
    environment:
      MODEL: /models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
      USE_MLOCK: "0"
    volumes:
      - ./model-cache:/models:ro
    ports: ["8001:8000"]
    restart: unless-stopped

  api:
    build:
      context: .
      dockerfile: ./api/Dockerfile
    command: ["uvicorn","api.main:app","--host","0.0.0.0","--port","8000"]
    environment:
      DATABASE_URL: postgresql+psycopg://postgres:pass@postgres:5432/postgres
      QDRANT_URL:  http://qdrant:6333
      OPENAI_BASE_URL: http://vllm:8000/v1
      OPENAI_API_KEY: dummy
    depends_on: [postgres, qdrant, vllm]
    ports: ["8000:8000"]

volumes:
  pgdata_named:
  qdrant_named:
  modelcache_named:
    driver: local
    driver_opts: { type: none, o: bind, device: /Volumes/LaCie/model-cache }
```

---

## 7 Troubleshooting notes

Symptom | Root cause | Fix
------- | ---------- | ---
`ValueError: Model path does not exist /models/...` in vllm logs | Model file not visible in the container | Ensure the model file is inside `./model-cache/` *and* the volume path matches.
Endless CMake compile loop inside vllm | Container tried to build llama-cpp because the file was still missing | Same as above; `docker compose up -d --force-recreate --no-deps vllm`.
Qdrant exits with *No such file or directory* on `raft_state.json` | Bind-mounted directory not write-able by UID 1000 | `sudo chown -R 1000:1000 ./qdrant` then recreate the service.
Postgres `pg_wal/archive_status` error | Data dir half-initialised | `rm -rf ./pgdata/*` (or the SSD path) and restart Postgres.

These remedies are baked into `setup.sh`, so a fresh clone rebuilds cleanly.

---

## 8 Next steps

* Flesh out the API routers in `api/routers/`.  
* Implement embedding & ingestion workers (see `workers/` stubs).  
* Swap TinyLlama for a larger model once you have a GPU box‚Äîonly the volume mapping changes.
